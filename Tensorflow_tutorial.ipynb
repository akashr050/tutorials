{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is the basic lego block in tensorflow. It is somewhat similar to arrays in numpy or dataframe in pandas. The rank of a tensor is its number of dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: <li>5 is a tensor with Rank 0 and shape [] <br/>\n",
    "    <li>[1, 2] is a tensor with rank 1 and shape [2] <br/>\n",
    "    <li>[[1, 2, 3], [3, 4, 5]] is a tensor with rank 2 and shape [2, 3] <br/>\n",
    "    <li>[[[1, 2, 3]], [[4, 5, 6]]] is a tensor with rank 3 and shape [2, 1, 3] <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the building block in tensorflow is <b>computational graph</b>. It can be thought of as a map of operations to be performed for the anlaysis<br/>\n",
    "So, the computational graphs can be thought of as black-box models which take some tensors as inputs and produces the relevant tensor outputs. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, each black-box model consists of many operations. We say that computational graph is a graph of nodes. Each node takes no/some tensors as input and produces an output. Hence, we can assume nodes to be equivalent to functions in python. Note: This node can be a constant as well which outputs a given value let's say 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of implementing TensorFlow Core can be broken down into two parts: \n",
    "<li>Building the computational graph<br/>\n",
    "<li>Running the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of building nodes in the computational graph is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "node1 = tf.constant(3.0)\n",
    "node2 = tf.constant([2, 3], tf.float32)\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: As we told earlier, the process of building and running the computational graph are kept as two separate process in tensorflow. Hence, when we are printing the nodes we can't print their value. Now, let's see how we can run the computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the computational graphs, we have to create a Session object and use its run method to evaluate the node values as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = tf.Session()\n",
    "print(s.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try something bit more involed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node3 = node1 + node2\n",
    "s.run(node3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Note: that the broadcasting rules of numpy are valid here</i> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we compared the nodes to functions, they should have the capabililty to take any input. To bring in this functionality, we have placeholder. Placeholder can be thought of as empty tensor whose value will be provided at the run of computational graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "node3 = a + b\n",
    "print(a, b, node3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we run the session, we will provide the values of placeholders by specifying their values using a dictionary. This dictionary is known as feed_dict in the Tensor world ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = {a: 3, b:[2, 3]}\n",
    "print(s.run(node3, fd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third type of tensors available in tensorflow are Variables. The value of variables can changes during the run of the computational graphs. They are generally used to define the parameters of the model which we want to train. Ex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([2.0], tf.float32)\n",
    "b = tf.Variable([0.0], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "print(W, b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major differences between constant and variable tensors.\n",
    "<li> The value of constant tensors can not change during the run of computational graph whereas the value of variable tensors change during the run of computational graph\n",
    "<li> The value of constant tensor is initialised when we define them whereas to initialise the variable tensors we have to call a special function 'tf.global_variables_initializer()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "s.run(init)\n",
    "print(s.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets give it a try and see it evaluates values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_model = W * x + b\n",
    "d = {x: [[1, 2], [3, 4]]}\n",
    "output = s.run(linear_model, d)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Note: The type conversion is a big pain in the ass in tensorflow. So, try to explicitly mention the type of every tensor. Also, note how we can pass array of any dimension as x in the feed_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still one thing missing from the overall basic architecture of a machine learning setup. We are not learning/training the weights currently. For that, we will require a 'y' placeholder and a loss function to improve the 'W' variable and 'b' tensors iteratively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "loss = tf.reduce_sum((linear_model - y)**2, axis = None)\n",
    "print(loss)\n",
    "print(s.run(loss, {x: [1, 2, 3], y: [3, 4, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to minimize this loss by using some kind of an optimizer algorithm like gradient descent or stochastic gradient descent. This process is achieved by using the train API in tensorflow. <p>If we are not using the tensorflow, we would have to calculate the gradients of the loss and then re-evaluate the value of W and b based on our algorithm. This process is very cumbersome, entirely mechanical and error-prone. The beauty of tensorflow is that you can implement your algorithm without worrying about the optimizing part. They have a variety of in-built functions to provide that functionality. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01) #0.01 is the learning rate\n",
    "e = optimizer.minimize(loss)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to read the above code is:<br>\n",
    "Take gradient descent as our optimizer with learning rate of 0.01. After this we specify the loss function 'loss' and the objective i.e to minimize the loss in the second line of code.Note, optimizer is a way to get gradient w.r.t to the variable tensors involed with the loss function. Hence, we need to specify the loss function and the objective in the second line. \n",
    "<p> After defining the optimizer and objective function, we just need to iterate through the input values long enough to train the values for W and b. This is what we achieve below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.run(init)\n",
    "for i in range(1000):\n",
    "    _, weights, b_value, l = s.run([e, W, b, loss], {x: [1, 2, 3, 4], y: [0, -1, -2, -3]})\n",
    "    # This piece is code is just to track the progress of the model  \n",
    "    if i % 100 == 0:\n",
    "        print(weights, b_value, l)\n",
    "\n",
    "print(s.run([W, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, we have trained the model, we would like to predict the y values based on x values. For this, we need to pick the best value of W and b and assign it to W and b. This can be achieved by using tf.assign function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.run(init)\n",
    "asg_W_best = tf.assign(W, weights)\n",
    "asg_b_best = tf.assign(b, b_value)\n",
    "s.run([asg_W_best, asg_b_best])\n",
    "print(s.run(loss, {x: [1, 2], y: [0, -1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Even after initializing the value of W and b at the top, we reassign the values of W and b in line 2 & 3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire code can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#==============================================================================\n",
    "# Building the computational graph\n",
    "#\n",
    "#==============================================================================\n",
    "\n",
    "# Initialising the tensors\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable([-1.0], tf.float32)\n",
    "b = tf.Variable([0.0], tf.float32)\n",
    "\n",
    "# Initialising the model and its loss function\n",
    "linear_model = x * W + b\n",
    "loss = tf.reduce_sum((linear_model - y)**2)\n",
    "\n",
    "# Defining the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "training = optimizer.minimize(loss)\n",
    "\n",
    "#==============================================================================\n",
    "# Running the computational graph\n",
    "#\n",
    "#==============================================================================\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "s = tf.Session()\n",
    "s.run(init)\n",
    "data = {x: [1,2, 3, 4], y: [3, 4, 5, 6]}\n",
    "for i in range(1000):\n",
    "    s.run(training, data)\n",
    "\n",
    "best_W, best_b, best_loss = s.run([W, b, loss], data)\n",
    "print(best_W, best_b, best_loss)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
